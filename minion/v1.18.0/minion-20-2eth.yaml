#cloud-config

passwd:
  users:
    - name: admin
      ssh_authorized_keys:
        - "ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA8PF9Cj1svhd8Cb+yFrw9GHA+6j6sq1halwXrVx+m+F49xMrCKV0zpdVJGwEb13U3WDaibhPPHy5dWCeVj7ZxSLDdo510p1NFAMicmWF9CNJ1oQF+uRu8IobaJxrlW00+rYJ4EjRLbFQf3X4OhCkjDcF3KS/uhmpX2niMj5ffcWN4PlglDaahA3YZXEG8BFQxJgmaOgJSq6M34wqTFdhBCC1beQPPtwDE0N4NYNS3lhl8n3Fe84m59DtD+O6xD2EL6II5fkfMqeFYmgi8M9f/kMSCdAWHtaahAFBNSILh5jOiY+OIYL7DSL4sbyjUa7vPGHaXKhfVfiZID0mwyr8Uzw== cent@centos-vm"
      groups:
        - "sudo"
        - "docker"

etcd:
  proxy: on
  version: 3.4.5
  client_cert_auth:            true
  peer_client_cert_auth:       true
  trusted_ca_file:             /etc/ssl/certs/ca.pem
  peer_trusted_ca_file:        /etc/ssl/certs/ca.pem
  cert_file:                   /var/lib/etcd/ssl/etcd-node.pem
  key_file:                    /var/lib/etcd/ssl/etcd-node-key.pem
  peer_cert_file:              /var/lib/etcd/ssl/etcd-node.pem
  peer_key_file:               /var/lib/etcd/ssl/etcd-node-key.pem
  listen-client-urls:          https://0.0.0.0:2379
  advertise-client-urls:       https://0.0.0.0:2379
  initial-cluster:             manager-01=https://10.9.56.101:2380,manager-02=https://10.9.56.102:2380,manager-03=https://10.9.56.103:2380
  auto_compaction_retention:   1
flannel:
  etcd_cafile:    /etc/ssl/certs/ca.pem
  etcd_certfile:  /var/lib/etcd/ssl/etcd-node.pem
  etcd_keyfile:   /var/lib/etcd/ssl/etcd-node-key.pem
  version:        0.12.0
  etcd_prefix:    "/coreos.com/network2"
  etcd_endpoints: "https://10.9.56.101:2379,https://10.9.56.102:2379,https://10.9.56.103:2379"
  interface:      ens32
locksmith:
  etcd_cafile:     /etc/ssl/certs/ca.pem
  etcd_certfile:   /var/lib/etcd/ssl/etcd-node.pem
  etcd_keyfile:    /var/lib/etcd/ssl/etcd-node-key.pem
  reboot_strategy: "off"
  etcd_endpoints:  "https://10.9.56.101:2379,https://10.9.56.102:2379,https://10.9.56.103:2379"
systemd:
  units:
     - name: "settimezone.service"
       enable: true
       contents: |
         [Service]
         ExecStart=/usr/bin/timedatectl set-timezone Asia/Jakarta
         Restart=on-failure
         RestartSec=5
         [Install]
         WantedBy=multi-user.target
     - name: mnt-nfs.mount
       enable: true
       contents: |
         [Unit]
         Description=Mount /mnt/nfs/
         Before=local-fs.target
         [Mount]
         What=/dev/sdb
         Where=/mnt/nfs
         Type=ext4
         [Install]
         WantedBy=local-fs.target
     - name: systemd-timesyncd.service
       mask: true
     - name: ntpd.service
       enable: true
     - name: "locksmithd.service"
       enable: true
       dropins:
        - name: "20-locksmithd-config.conf"
          contents: |
            [Service]
            Environment="LOCKSMITHD_ETCD_CAFILE=/etc/ssl/certs/ca.pem"
            Environment="LOCKSMITHD_ETCD_CERTFILE=/var/lib/etcd/ssl/etcd-node.pem"
            Environment="LOCKSMITHD_ETCD_KEYFILE=/var/lib/etcd/ssl/etcd-node-key.pem"
            Environment="LOCKSMITHD_ENDPOINT=https://10.9.56.101:2379,https://10.9.56.102:2379,https://10.9.56.103:2379"
     - name: "etcd-member.service"
       enable: true
       dropins:
        - name: conf1.conf
          contents: |
            [Service]
            EnvironmentFile=/etc/flannel/options.env
        - name: "20-cl-etcd-member.conf"
          contents: |
            [Service]
            Environment="ETCD_NAME=worker-020"
            Environment="ETCD_DATA_DIR=/var/lib/etcd"
            Environment="ETCD_ENABLE_V2=true"
            Environment="ETCD_OPTS=--trusted-ca-file /etc/ssl/certs/ca.pem \
               --cert-file /var/lib/etcd/ssl/etcd-node.pem \
               --key-file /var/lib/etcd/ssl/etcd-node-key.pem \
               --peer-trusted-ca-file /etc/ssl/certs/ca.pem \
               --peer-cert-file /var/lib/etcd/ssl/etcd-node.pem \
               --peer-key-file /var/lib/etcd/ssl/etcd-node-key.pem \
               --client-cert-auth \
               --peer-client-cert-auth \
               --listen-client-urls https://0.0.0.0:2379 \
               --advertise-client-urls https://0.0.0.0:2379 \
               --initial-cluster manager-01=https://10.9.56.101:2380,manager-02=https://10.9.56.102:2380,manager-03=https://10.9.56.103:2380 \
               --proxy on \
               --log-level info \
               --logger zap \
               --log-outputs stderr \
               --auto-compaction-retention 1"
     - name: "grpc-proxy-0.service"
       enable: true
       contents: |
         [Unit]
         Description=grpc-0
         Requires=etcd-v3-datastore.service
         After=etcd-v3-datastore.service
         [Service]
         Type=notify
         Restart=always
         RestartSec=5s
         Environment="ETCD_IMAGE_TAG=v3.4.5"
         Environment="ETCD_USER=etcd"
         Environment="ETCD_DATA_DIR=/var/lib/etcd"
         ExecStart=/usr/lib/coreos/etcd-wrapper grpc-proxy start --listen-addr=10.9.56.123:23790 \
           --endpoints=https://kubernetes.example.com:2379 \
           --advertise-client-url=10.9.56.123:23790 \
           --resolver-prefix=https://kubernetes.example.com:2379 \
           --resolver-ttl=60 \
           --namespace=worker-020
         [Install]
         WantedBy=multi-user.target
     - name: "grpc-proxy-1.service"
       enable: true
       contents: |
         [Unit]
         Description=grpc-1
         Requires=etcd-v3-datastore.service 
         After=etcd-v3-datastore.service
         [Service]
         Type=notify
         Restart=always
         RestartSec=5s
         Environment="ETCD_IMAGE_TAG=v3.4.5"
         Environment="ETCD_USER=etcd"
         Environment="ETCD_DATA_DIR=/var/lib/etcd"
         ExecStart=/usr/lib/coreos/etcd-wrapper grpc-proxy start --listen-addr=10.9.56.123:23791 \
           --endpoints=https://kubernetes.example.com:2379 \
           --advertise-client-url=10.9.56.123:23791 \
           --resolver-prefix=https://kubernetes.example.com:2379 \
           --resolver-ttl=60 \
           --namespace=worker-020
         [Install]
         WantedBy=multi-user.target
     - name: "etcd-v3-datastore.service"
       enable: true
       contents: |
         [Unit]
         Description=Choose etcd v3 datastore
         After=etcd-member.service
         Requires=etcd-member.service
         [Service]
         ExecStart=/etc/profile.d/etcdctl.sh
         RemainAfterExit=yes
         Type=oneshot
     - name: "locksmith-profile.service"
       enable: true
       contents: |
         [Unit]
         Description=locksmith profile
         [Service]
         ExecStart=/etc/profile.d/locksmithctl.sh
         RemainAfterExit=yes
         Type=oneshot
     - name: "rpcbind.service"
       enable: true
     - name: "rpc-statd.service"
       enable: true
     - name: "nfsd.service"
       enable: true
     - name: "timer-ten-minutes-nfsd.timer"
       enable: true
       contents: |
         [Unit]
         Description=10 minutes timer (600 seconds)
         [Timer]
         # Time to wait after booting before we run first time
         OnBootSec=5min
         # Time between running each consecutive time
         OnUnitActiveSec=10min
         Unit=timer-ten-minutes-nfsd.target
         [Install]
         WantedBy=basic.target
     - name: "timer-ten-minutes-nfsd.target"
       enable: true
       contents: |
         [Unit]
         Description=10 minutes timer (600 seconds)
         StopWhenUnneeded=yes
     - name: "refresh-nfsd.service"
       enable: true
       contents: |
         [Unit]
         Description=Installs cni tools
         Wants=timer-ten-minutes-nfsd.timer
         [Service]
         ExecStart=sudo /opt/bin/scripts/refresh-nfsd.sh
         [Install]
         WantedBy=imer-ten-minutes-nfsd.target
     - name: "flanneld.service"
       enable: true
       dropins:
         - name: "50-ssl.conf"
           contents: |
             [Unit]
             Description=Flannel etcd ssl dir
             Requires=etcd-member.service etcd-v3-datastore.service
             After=etcd-member.service etcd-v3-datastore.service
             [Service]
             Environment="ETCD_SSL_DIR=/var/lib/etcd/ssl"
         - name: "50-network-config.conf"
           contents: |
             [Unit]
             Description=Flannel networking vxlan
             Requires=etcd-member.service etcd-v3-datastore.service
             After=etcd-member.service etcd-v3-datastore.service
             [Service]
             Environment="FLANNEL_IMAGE_TAG=v0.12.0-amd64"
             Environment="FLANNEL_IMAGE_URL=quay.io/coreos/flannel"
             Environment="FLANNEL_ETCD=https://kubernetes.example.com:2379"
             Environment="FLANNEL_ETCD_KEY=/coreos.com/network2"
             ExecStartPre=ETCDCTL_API=2 /usr/bin/etcdctl --ca-file=/etc/ssl/certs/ca.pem --cert-file=/var/lib/etcd/ssl/etcd-node.pem --key-file=/var/lib/etcd/ssl/etcd-node-key.pem --endpoints="https://kubernetes.example.com:2379" set /coreos.com/network2/config '{ "Network":"10.244.0.0/16", "SubnetLen": 24, "SubnetMin": "10.244.0.0", "SubnetMax":"10.244.255.0", "Backend": {"Type": "vxlan"} }'
             
     - name: "docker-tcp.socket"
       enable: true
       contents: |
         [Unit]
         Description=Docker Socket for the API
         [Socket]
         ListenStream=2375
         Service=docker.service
         BindIPv6Only=both
         [Install]
         WantedBy=sockets.target
     - name: "dns.service"
       enable: true
       contents: |
         [Unit]
         Description=storytel/dnsmasq
         Requires=docker.service
         After=docker.service
         [Service]
         Restart=always
         ExecStartPre=-/usr/bin/docker rm dnsmasq
         ExecStart=/usr/bin/docker run --name dnsmasq --net=host \
           -v /etc/dnsmasq:/etc/dnsmasq \
           --cap-add=NET_ADMIN \
           storytel/dnsmasq
         ExecStop=-/usr/bin/docker stop dnsmasq
         ExecStopPost=-/usr/bin/docker rm dnsmasq
         [Install]
         WantedBy=multi-user.target
     - name: "setup-network-environment.service"
       enable: true
       contents: |
         [Unit]
         Description=Setup Network Environment
         Documentation=http://github.com/kelseyhightower/setup-network-environment
         Requires=network-online.target cni-install.service
         After=network-online.target cni-install.service
         [Service]
         ExecStart=/opt/bin/setup-network-environment
         Restart=on-failure
         RestartSec=5
         [Install]
         WantedBy=multi-user.target
     - name: "cni-install.service"
       enable: true
       contents: |
         [Unit]
         Description=Installs cni tools
         [Service]
         ExecStart=/opt/bin/cni-install.sh
         RemainAfterExit=yes
         Type=oneshot
     - name: "kube-kubelet.service"
       enable: true
       contents: |
         [Unit]
         Description=Kubernetes Kubelet
         Documentation=https://github.com/kubernetes/kubernetes
         Requires=setup-network-environment.service
         After=setup-network-environment.service
         [Service]
         Environment=no_proxy=localhost,127.0.0.0/8,127.0.0.1,::1,10.9.56.101,10.9.56.102,10.9.56.103,manager-01,manager-02,manager-03,manager-01.example.com,manager-02.example.com,manager-03.example.com,example.com,/var/run/docker.sock
         EnvironmentFile=/etc/network-environment
         # wait for kubernetes master to be up and ready
         ExecStart=/opt/bin/kubelet \
           --hostname-override=worker-020.example.com \
           --config="/var/lib/kubelet/kubelet-config.yaml" \
           --cert-dir="/etc/kubernetes/ssl" \
           --image-pull-progress-deadline=2m \
           --lock-file="/var/run/lock/kubelet.lock" \
           --exit-on-lock-contention \
           --logtostderr="true" \
           --register-node="true" \
           --node-labels="label=swarm" \
           --v=3
         Restart=on-failure
         RestartSec=5
         [Install]
         WantedBy=multi-user.target
     - name: "kube-proxy.service"
       enable: true
       contents: |
         [Unit]
         Description=Kubernetes Proxy
         Documentation=https://github.com/kubernetes/kubernetes
         Requires=setup-network-environment.service
         After=setup-network-environment.service
         [Service]
         # wait for kubernetes master to be up and ready
         ExecStart=/opt/bin/kube-proxy \
           --hostname-override=worker-020.example.com \
           --config="/var/lib/kubeproxy/kube-proxy-config.yaml" \
           --bind-address=0.0.0.0 \
           --cluster-cidr=10.244.0.0/17 \
           --logtostderr="true" \
           --master=https://kubernetes.example.com:6443 \
           --proxy-mode="ipvs" \
           --masquerade-all \
           --v=3
         Restart=on-failure
         RestartSec=5
         [Install]
         WantedBy=multi-user.target
     - name: "vmware-tools.service"
       enable: true
       contents: |
         [Unit]
         Description=VMWare Tools
         [Service]
         Restart=always
         ExecStartPre=-/usr/bin/docker rm vmware-tools
         ExecStart=/usr/bin/docker run --net=host --privileged --name vmware-tools sergeyzh/vmware-tools
         ExecStop=-/usr/bin/docker stop vmware-tools
         ExecStopPost=-/usr/bin/docker rm vmware-tools
         [Install]
         WantedBy=multi-user.target
     - name: "portainer.service"
       enable: true
       contents: |
         [Unit]
         Description=portainer
         Requires=docker.service
         After=docker.service
         [Service]
         Restart=always
         ExecStartPre=-/usr/bin/docker rm portainer
         ExecStart=/usr/bin/docker run --net=host --privileged \
           -v /var/run/docker.sock:/var/run/docker.sock \
           -v /etc/portainer/data:/data \
           --name portainer portainer/portainer
         ExecStop=-/usr/bin/docker stop portainer
         ExecStopPost=-/usr/bin/docker rm portainer
         [Install]
         WantedBy=multi-user.target
     - name: "snmpd.service"
       enable: true
       contents: |
         [Unit]
         Description=snmpd
         Requires=docker.service
         After=docker.service
         [Service]
         Restart=always
         ExecStartPre=-/usr/bin/docker rm snmpd
         ExecStart=/usr/bin/docker run --privileged \
           -v /etc/snmpd.conf:/etc/snmp/snmpd.conf \
           -v /proc:/host_proc \
           -p 161:161/udp \
           --name snmpd really/snmpd
         ExecStop=-/usr/bin/docker stop snmpd
         ExecStopPost=-/usr/bin/docker rm snmpd
         [Install]
         WantedBy=multi-user.target
update:
    reboot-strategy: "off"
    group: "stable"
    server: "https://public.update.flatcar-linux.net/v1/update/"
networkd:
  units:
     - name: "00-eth0.network"
       contents: |
         [Match]
         Name=ens32
         [Network]
         DNS=10.100.0.10
         DNS=x.x.x.x
         DNS=x.x.x.x
         Domains=default.svc.cluster.local
         Domains=svc.cluster.local
         Domains=cluster.local
         Domains=example.com
         Address=10.9.56.123/24
         Gateway=10.9.56.1
     - name: "00-eth1.network"
       contents: |
         [Match]
         Name=ens33
         [Network]
         Address=192.168.56.23/24
storage:
  filesystems:
    - name: nfs
      mount:
        device: /dev/sdb
        format: ext4
        wipe_filesystem: true
  files:
    - filesystem: "root"
      path: "/etc/motd"
      mode: 0644
      contents:
        inline: |
          ================================================
          --- Kubernetes Minions-020                   ---
          --- inet & etcd ip:        10.9.56.123       ---
          --- private ip:            192.168.56.23     ---
          ================================================
    
    - filesystem: "root"
      path: "/etc/nsswitch.conf"
      mode: 0644
      contents:
        inline: |
          # /etc/nsswitch.conf:

          passwd:      files usrfiles
          shadow:      files usrfiles
          group:       files usrfiles

          hosts:       files usrfiles resolve dns
          networks:    files usrfiles dns

          services:    files usrfiles
          protocols:   files usrfiles
          rpc:         files usrfiles

          ethers:      files
          netmasks:    files
          netgroup:    files
          bootparams:  files
          automount:   files
          aliases:     files
    
    - filesystem: "root"
      path: "/etc/dnsmasq/dnsmasq.conf"
      mode: 0644
      contents:
        inline: |
          domain-needed
          bogus-priv
          no-hosts
          keep-in-foreground
          no-resolv
          expand-hosts
          server=10.100.0.10
          server=x.x.x.x
          server=x.x.x.x
          server=/svc.cluster.local/10.100.0.10
          
    - filesystem: "root"
      path: "/etc/hostname"
      mode: 0644
      contents:
        inline: worker-020.example.com
          
    - filesystem: "root"
      path: "/opt/bin/add-iptables.sh"
      mode: 0755
      contents:
        inline: |
          #! /usr/bin/bash
          sudo iptables -P FORWARD ACCEPT
          
    - filesystem: "root"
      path: "/opt/bin/scripts/refresh-nfsd.sh"
      mode: 0755
      contents:
        inline: |
          #! /usr/bin/bash
          sudo systemctl start nfsd.service
          
    - filesystem: "root"
      path: "/etc/conf.d/nfs"
      mode: 0644
      contents:
        inline: |
          OPTS_RPC_MOUNTD=""
    
    - filesystem: "root"
      path: "/etc/exports"
      mode: 0644
      contents:
        inline: |
          # Change /mnt/nfs/ with the dir you want to export over nfs
          /mnt/nfs/ 10.9.56.0/24(rw,async,insecure,no_subtree_check,no_root_squash,fsid=0)
          /mnt/nfs/ 10.244.0.0/16(rw,async,insecure,no_subtree_check,no_root_squash,fsid=0)
          
    - filesystem: "root"
      path: "/var/log/audit.log"
      mode: 0644
    
    - filesystem: "root"
      path: "/var/log/container.log"
      mode: 0644
    
    - path: /etc/snmpd.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          ###############################################################################
          #
          # EXAMPLE.conf:
          #   An example configuration file for configuring the Net-SNMP agent ('snmpd')
          #   See the 'snmpd.conf(5)' man page for details
          #
          #  Some entries are deliberately commented out, and will need to be explicitly activated
          #
          ###############################################################################
          #
          #  AGENT BEHAVIOUR
          #
          
          #  Listen for connections from the local system only
          #agentAddress  udp:127.0.0.1:161
          #  Listen for connections on all interfaces (both IPv4 *and* IPv6)
          agentAddress udp:161
          
          
          
          ###############################################################################
          #
          #  SNMPv3 AUTHENTICATION
          #
          #  Note that these particular settings don't actually belong here.
          #  They should be copied to the file /var/net-snmp/snmpd.conf
          #     and the passwords changed, before being uncommented in that file *only*.
          #  Then restart the agent
          
          #  createUser authOnlyUser  MD5 "remember to change this password"
          #  createUser authPrivUser  SHA "remember to change this one too"  DES
          #  createUser internalUser  MD5 "this is only ever used internally, but still change the password"
          
          #  If you also change the usernames (which might be sensible),
          #  then remember to update the other occurances in this example config file to match.
          
          
          
          ###############################################################################
          #
          #  ACCESS CONTROL
          #
          
                                                           #  system + hrSystem groups only
          view   systemonly  included   .1.3.6.1.2.1.1
          view   systemonly  included   .1.3.6.1.2.1.25.1
          view   all	     included   .1
          
                                                           #  Full access from the local host
          #rocommunity public  localhost
                                                           #  Default access to basic system info
           rocommunity public     default    -V systemonly
           rocommunity Pl4ym3d1a  default    -V all
          
                                                           #  Full access from an example network
                                                           #     Adjust this network address to match your local
                                                           #     settings, change the community string,
                                                           #     and check the 'agentAddress' setting above
          #rocommunity secret  10.0.0.0/16
          
                                                           #  Full read-only access for SNMPv3
           rouser   authOnlyUser
                                                           #  Full write access for encrypted requests
                                                           #     Remember to activate the 'createUser' lines above
          #rwuser   authPrivUser   priv
          
          #  It's no longer typically necessary to use the full 'com2sec/group/access' configuration
          #  r[ou]user and r[ow]community, together with suitable views, should cover most requirements
          
          
          
          ###############################################################################
          #
          #  SYSTEM INFORMATION
          #
          
          #  Note that setting these values here, results in the corresponding MIB objects being 'read-only'
          #  See snmpd.conf(5) for more details
          sysLocation    Sitting on the Dock of the Bay
          sysContact     Me <me@example.org>
                                                           # Application + End-to-End layers
          sysServices    72
          
          
          #
          #  Process Monitoring
          #
                                         # At least one  'mountd' process
          proc  mountd
                                         # No more than 4 'ntalkd' processes - 0 is OK
          proc  ntalkd    4
                                         # At least one 'sendmail' process, but no more than 10
          proc  sendmail 10 1
          
          #  Walk the UCD-SNMP-MIB::prTable to see the resulting output
          #  Note that this table will be empty if there are no "proc" entries in the snmpd.conf file
          
          
          #
          #  Disk Monitoring
          #
                                         # 10MBs required on root disk, 5% free on /var, 10% free on all other disks
          disk       /     10000
          disk       /var  5%
          includeAllDisks  10%
          
          #  Walk the UCD-SNMP-MIB::dskTable to see the resulting output
          #  Note that this table will be empty if there are no "disk" entries in the snmpd.conf file
          
          
          #
          #  System Load
          #
                                         # Unacceptable 1-, 5-, and 15-minute load averages
          load   12 10 5
          
          #  Walk the UCD-SNMP-MIB::laTable to see the resulting output
          #  Note that this table *will* be populated, even without a "load" entry in the snmpd.conf file
          
          
          
          ###############################################################################
          #
          #  ACTIVE MONITORING
          #
          
                                              #   send SNMPv1  traps
           trapsink     localhost public
                                              #   send SNMPv2c traps
          #trap2sink    localhost public
                                              #   send SNMPv2c INFORMs
          #informsink   localhost public
          
          #  Note that you typically only want *one* of these three lines
          #  Uncommenting two (or all three) will result in multiple copies of each notification.
          
          
          #
          #  Event MIB - automatically generate alerts
          #
                                             # Remember to activate the 'createUser' lines above
          iquerySecName   internalUser       
          rouser          internalUser
                                             # generate traps on UCD error conditions
          defaultMonitors          yes
                                             # generate traps on linkUp/Down
          linkUpDownNotifications  yes
          
          
          
          ###############################################################################
          #
          #  EXTENDING THE AGENT
          #
          
          #
          #  Arbitrary extension commands
          #
           extend    test1   /bin/echo  Hello, world!
           extend-sh test2   echo Hello, world! ; echo Hi there ; exit 35
          #extend-sh test3   /bin/sh /tmp/shtest
          
          #  Note that this last entry requires the script '/tmp/shtest' to be created first,
          #    containing the same three shell commands, before the line is uncommented
          
          #  Walk the NET-SNMP-EXTEND-MIB tables (nsExtendConfigTable, nsExtendOutput1Table
          #     and nsExtendOutput2Table) to see the resulting output
          
          #  Note that the "extend" directive supercedes the previous "exec" and "sh" directives
          #  However, walking the UCD-SNMP-MIB::extTable should still returns the same output,
          #     as well as the fuller results in the above tables.
          
          
          #
          #  "Pass-through" MIB extension command
          #
          #pass .1.3.6.1.4.1.8072.2.255  /bin/sh       PREFIX/local/passtest
          #pass .1.3.6.1.4.1.8072.2.255  /usr/bin/perl PREFIX/local/passtest.pl
          
          # Note that this requires one of the two 'passtest' scripts to be installed first,
          #    before the appropriate line is uncommented.
          # These scripts can be found in the 'local' directory of the source distribution,
          #     and are not installed automatically.
          
          #  Walk the NET-SNMP-PASS-MIB::netSnmpPassExamples subtree to see the resulting output
          
          
          #
          #  AgentX Sub-agents
          #
                                                     #  Run as an AgentX master agent
           master          agentx
                                                     #  Listen for network connections (from localhost)
                                                     #    rather than the default named socket /var/agentx/master
          #agentXSocket    tcp:localhost:705
          
    - filesystem: "root"
      path: "/opt/bin/cni-install.sh"
      mode: 0755
      contents:
        inline: |
          #! /usr/bin/bash
          if [ ! -f /opt/bin/setup-network-environment ]; then
            sudo mkdir -p /etc/kubernetes/kubelet-plugins/volume/exec/
            sudo mkdir -p /var/etcd/data/
            sudo mkdir -p /etc/docker/
            sudo mkdir -p /etc/kube-flannel/
            sudo mkdir -p /etc/sysconfig/
            sudo mkdir -p /etc/portainer/data/
            sudo mkdir -p /coreos.com/network2/
            sudo mkdir -p /var/lib/etcd/ssl/
            sudo mkdir -p /var/lib/kubeproxy/
            sudo mkdir -p /var/lib/kubelet/
            sudo mkdir -p /mnt/nfs/
            sudo mkdir -p /etc/ssl/certs/
            sudo mkdir -p /etc/ssl/etcd/
            sudo mkdir -p /etc/dnsmasq/
            sudo mkdir -p /run/dbus/
            sudo mkdir -p /opt/bin/
            sudo mkdir -p /etc/coredns/tls/etcd/
            sudo mkdir -p /opt/bin/scripts/
            sudo mkdir -p /etc/kubernetes/cni/net.d/
            sudo mkdir -p /etc/rkt/net.d/
            sudo mkdir -p /etc/cni/net.d/
            sudo mkdir -p /opt/plugins/
            sudo mkdir -p /opt/cni/bin/
            sudo mkdir -p /etc/kubernetes/manifests/
            sudo mkdir -p /etc/kubernetes/ssl/
            sudo mkdir -p /etc/kubernetes/pki/
            sudo mkdir -p /etc/kubernetes/addons/
            sudo mkdir -p /opt/plugins/usr/local/sbin/
            sudo mkdir -p /opt/plugins/usr/local/bin/
            sudo mkdir -p /opt/plugins/etc/systemd/system/
            sudo mkdir -p /opt/prometheus/{conf,data}
            sudo chown 65534:65534 /opt/prometheus/data
            sudo mkdir -p /data/grafana/data/
            sudo mkdir -p /data/logstash/data/
            sudo mkdir -p /data/elasticsearch/data/
            sudo chown -R 1000.1000 /data
            echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf && sudo sysctl -p
            echo "CNI not installed - installing."
            export K8S_VERSION=v1.18.0
            sudo wget -q -N -P /opt/bin --show-progress --https-only --timestamping \
                 https://storage.googleapis.com/kubernetes-release/release/${K8S_VERSION}/bin/linux/amd64/kubelet \
                 https://storage.googleapis.com/kubernetes-release/release/${K8S_VERSION}/bin/linux/amd64/kube-proxy \
                 https://github.com/astronomer/astro-cli/releases/download/v0.12.0/astro_0.12.0_linux_amd64.tar.gz
            sudo chmod +x /opt/bin/kubelet /opt/bin/kube-proxy
            sudo wget -q -N -P /opt/bin --show-progress --https-only --timestamping \
                 https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64
            sudo wget -q -N -P /opt/bin --show-progress --timestamping \
                 http://github.com/kelseyhightower/setup-network-environment/releases/download/1.0.1/setup-network-environment
            sudo chmod +x /opt/bin/setup-network-environment
            sudo wget -q -N -P /opt/plugins --show-progress --https-only --timestamping \
                 https://github.com/containernetworking/plugins/releases/download/v0.8.5/cni-plugins-linux-amd64-v0.8.5.tgz \
                 https://github.com/containernetworking/cni/releases/download/v0.6.0/cni-amd64-v0.6.0.tgz
            sudo tar -xvf /opt/plugins/cni-plugins-linux-amd64-v0.8.5.tgz -C /opt/cni/bin/
            sudo tar -xvf /opt/plugins/cni-amd64-v0.6.0.tgz -C /opt/cni/bin/
            sudo tar -xvf /opt/bin/astro_0.12.0_linux_amd64.tar.gz
            sudo chmod +x /opt/bin/astro
            sudo curl -L https://github.com/docker/compose/releases/download/1.25.4/docker-compose-`uname -s`-`uname -m` -o /opt/bin/docker-compose
            sudo chmod +x /opt/bin/docker-compose
            sudo mv /opt/bin/jq-linux64 /opt/bin/jq
            sudo chmod +x /opt/bin/jq
            sudo systemctl start user@0.service
            sudo systemctl start locksmith-profile.service
            sudo systemctl start rpc-statd.service
            sudo systemctl start nfsd.service
            sudo systemctl start clean-dangling.service
            sudo /opt/bin/add-iptables.sh
            sudo sysctl -w vm.max_map_count=262144
          else
            sudo systemctl start user@0.service
            sudo systemctl start etcd-v3-datastore.service
            sudo systemctl start locksmith-profile.service
            sudo systemctl start rpc-statd.service
            sudo systemctl start nfsd.service
            sudo systemctl start clean-dangling.service
            sudo /opt/bin/add-iptables.sh
            sudo sysctl -w vm.max_map_count=262144
          fi
          
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/ca-key.pem"
      mode: 0644
      contents:
        inline: |
          -----BEGIN RSA PRIVATE KEY-----
          -----END RSA PRIVATE KEY-----
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/ca.pem"
      mode: 0644
      contents:
        inline: |
          -----BEGIN CERTIFICATE-----
          -----END CERTIFICATE-----
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/etcd-node-key.pem"
      mode: 0644
      contents:
        inline: |
          -----BEGIN RSA PRIVATE KEY-----
          -----END RSA PRIVATE KEY-----
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/etcd-node.pem"
      mode: 0644
      contents:
        inline: |
          -----BEGIN CERTIFICATE-----
          -----END CERTIFICATE-----
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/kube-proxy-worker-key.pem"
      mode: 0644
      contents:
        inline: |
          -----BEGIN RSA PRIVATE KEY-----
          -----END RSA PRIVATE KEY-----
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/kube-proxy-worker.pem"
      mode: 0644
      contents:
        inline: |
          -----BEGIN CERTIFICATE-----
          -----END CERTIFICATE-----

    - filesystem: "root"
      path: "/etc/flannel/options.env"
      mode: 0755
      contents:
        inline: |
          ETCDCTL_CA_FILE=/etc/ssl/certs/ca.pem
          ETCDCTL_CERT_FILE=/var/lib/etcd/ssl/etcd-node.pem
          ETCDCTL_KEY_FILE=/var/lib/etcd/ssl/etcd-node-key.pem
          ETCDCTL_ENDPOINTS="https://10.9.56.101:2379,https://10.9.56.102:2379,https://10.9.56.103:2379"
          FLANNELD_IFACE="ens32"
          FLANNELD_ETCD_ENDPOINTS="https://10.9.56.101:2379,https://10.9.56.102:2379,https://10.9.56.103:2379"
          FLANNELD_ETCD_PREFIX="/coreos.com/network2"
          no_proxy=localhost,127.0.0.0/8,127.0.0.1,::1,10.9.56.101,10.9.56.102,10.9.56.103,manager-01,manager-02,manager-03,manager-01.example.com,manager-02.example.com,manager-03.example.com,example.com,/var/run/docker.sock
    
    - filesystem: "root"
      path: "/run/systemd/system/etcd.service.d/30-certificates.conf"
      mode: 0644
      contents:
        inline: |
          [Service]
          # Client Env Vars
          Environment=ETCD_TRUSTED_CA_FILE=/etc/ssl/certs/ca.pem
          Environment=ETCD_CA_FILE=/etc/ssl/certs/ca.pem
          Environment=ETCD_CERT_FILE=/var/lib/etcd/ssl/etcd-node.pem
          Environment=ETCD_KEY_FILE=/var/lib/etcd/ssl/etcd-node-key.pem
          Environment=ETCD_DATA_DIR=/var/lib/etcd
          # Peer Env Vars
          Environment=ETCD_PEER_TRUSTED_CA_FILE=/etc/ssl/certs/ca.pem
          Environment=ETCD_PEER_CA_FILE=/etc/ssl/certs/ca.pem
          Environment=ETCD_PEER_CERT_FILE=/var/lib/etcd/ssl/etcd-node.pem
          Environment=ETCD_PEER_KEY_FILE=/var/lib/etcd/ssl/etcd-node-key.pem
    
    - filesystem: "root"
      path: "/etc/hosts"
      mode: 0644
      contents:
        inline: |
          # /etc/hosts: Local Host Database
          #
          # This file describes a number of aliases-to-address mappings for the for 
          # local hosts that share this file.
          #
          # The format of lines in this file is:
          #
          # IP_ADDRESS    canonical_hostname      [aliases...]
          #
          #The fields can be separated by any number of spaces or tabs.
          #
          # In the presence of the domain name service or NIS, this file may not be 
          # consulted at all; see /etc/host.conf for the resolution order.
          #
          
          # IPv4 and IPv6 localhost aliases
          127.0.0.1         localhost
          ::1               localhost
          10.9.56.101       manager-01.example.com
          10.9.56.102       manager-02.example.com
          10.9.56.103       manager-03.example.com
          10.9.56.104       worker-001.example.com
          10.9.56.105       worker-002.example.com
          10.9.56.106       worker-003.example.com
          10.9.56.107       worker-004.example.com
          10.9.56.108       worker-005.example.com
          10.9.56.109       worker-006.example.com
          10.9.56.110       worker-007.example.com
          10.9.56.111       worker-008.example.com
          10.9.56.112       worker-009.example.com
          10.9.56.113       worker-010.example.com
          10.9.56.114       worker-011.example.com
          10.9.56.115       worker-012.example.com
          10.9.56.116       worker-013.example.com
          10.9.56.117       worker-014.example.com
          10.9.56.118       worker-015.example.com
          10.9.56.119       worker-016.example.com
          10.9.56.120       worker-017.example.com
          10.9.56.121       worker-018.example.com
          10.9.56.122       worker-019.example.com
          10.9.56.123       worker-020.example.com
          
          #
          # Imaginary network.
          #10.0.0.2               myname
          #10.0.0.3               myfriend
          #
          # According to RFC 1918, you can use the following IP networks for private 
          # nets which will never be connected to the Internet:
          #
          #       10.0.0.0        -   10.255.255.255
          #       172.16.0.0      -   172.31.255.255
          #       192.168.0.0     -   192.168.255.255
          #
          # In case you want to be able to connect directly to the Internet (i.e. not 
          # behind a NAT, ADSL router, etc...), you need real official assigned 
          # numbers.  Do not try to invent your own network numbers but instead get one 
          # from your network provider (if any) or from your regional registry (ARIN, 
          # APNIC, LACNIC, RIPE NCC, or AfriNIC.)
          #
          
    - filesystem: "root"
      path: "/etc/profile.d/locksmithctl.sh"
      mode: 0755
      contents:
        inline: |
          #! /usr/bin/bash
          export LOCKSMITHCTL_ETCD_CAFILE=/etc/ssl/certs/ca.pem
          export LOCKSMITHCTL_ETCD_CERTFILE=/var/lib/etcd/ssl/etcd-node.pem
          export LOCKSMITHCTL_ETCD_KEYFILE=/var/lib/etcd/ssl/etcd-node-key.pem
          export LOCKSMITHCTL_ENDPOINT="https://10.9.56.101:2379,https://10.9.56.102:2379,https://10.9.56.103:2379"
    
    - filesystem: "root"
      path: "/etc/profile.d/etcdctl.sh"
      mode: 0755
      contents:
        inline: |
          #! /usr/bin/bash
          export ETCD_TRUSTED_CA_FILE=/etc/ssl/certs/ca.pem
          export ETCD_DATA_DIR=/var/lib/etcd
          
    - filesystem: "root"
      path: "/opt/bin/scripts/check_etcdv3_status.sh"
      mode: 0755
      contents:
        inline: |
          #! /usr/bin/bash
          echo "ETCDCTL_API=3 etcdctl --cacert=/etc/ssl/certs/ca.pem --cert=/var/lib/etcd/ssl/etcd-node.pem --key=/var/lib/etcd/ssl/etcd-node-key.pem --endpoints="https://kubernetes.example.com:2379" member list --write-out table"
          ETCDCTL_API=3 etcdctl --cacert=/etc/ssl/certs/ca.pem --cert=/var/lib/etcd/ssl/etcd-node.pem --key=/var/lib/etcd/ssl/etcd-node-key.pem --endpoints="https://kubernetes.example.com:2379" member list --write-out table
          echo "===================================================================================================================================================================================="
          echo "ETCDCTL_API=3 etcdctl --cacert=/etc/ssl/certs/ca.pem --cert=/var/lib/etcd/ssl/etcd-node.pem --key=/var/lib/etcd/ssl/etcd-node-key.pem --endpoints="https://kubernetes.example.com:2379" version"
          echo "===================================================================================================================================================================================="
          ETCDCTL_API=3 etcdctl --cacert=/etc/ssl/certs/ca.pem --cert=/var/lib/etcd/ssl/etcd-node.pem --key=/var/lib/etcd/ssl/etcd-node-key.pem --endpoints="https://kubernetes.example.com:2379" version
          echo "============================"
          echo "sudo rkt list | grep running"
          echo "============================"
          sudo rkt list | grep running
    
    - filesystem: "root"
      path: "/opt/bin/scripts/check_service_status.sh"
      mode: 0755
      contents:
        inline: |
          #! /usr/bin/bash
          echo "======================================="
          echo "systemctl --type=service | grep running"
          echo "======================================="
          systemctl --type=service | grep running
          echo "=========================================="
          echo "systemctl --type=service | grep -v running"
          echo "=========================================="
          systemctl --type=service | grep -v running
    
    - filesystem: "root"
      path: "/opt/bin/scripts/check_iptables_status.sh"
      mode: 0755
      contents:
        inline: |
          #! /usr/bin/bash
          echo "======================================="
          echo "sudo iptables -t nat -L -n"
          echo "======================================="
          sudo iptables -t nat -L -n
          echo "======================================="
          echo "sudo netstat -tunlp"
          echo "======================================="
          sudo netstat -tunlp
          echo "======================================="
          echo "sudo ip route"
          echo "======================================="
          sudo ip route
          echo "======================================="
          echo "sudo route"
          echo "======================================="
          sudo route
          echo "======================================="
          echo "sudo netstat -i"
          echo "======================================="
          sudo netstat -i
          echo "======================================="
          echo "sudo brctl show"
          echo "======================================="
          sudo brctl show
    
    - filesystem: "root"
      path: "/opt/bin/scripts/check_ipvsadm_status.sh"
      mode: 0755
      contents:
        inline: |
          #! /usr/bin/bash
          echo "======================================"
          echo "sudo ipvsadm -ln"
          echo "======================================"
          sudo ipvsadm -ln
          echo "======================================"
          echo "sudo ipvsadm -l -c"
          echo "======================================"
          sudo ipvsadm -l -c
          echo "========================================"
          echo "sudo ipvsadm -l --timeout"
          echo "========================================"
          sudo ipvsadm -l --timeout
          echo "========================================"
          echo "sudo ipvsadm -l --stats"
          echo "========================================"
          sudo ipvsadm -l --stats
          echo "========================================"
          echo "sudo ipvsadm -l --rate"
          echo "========================================"
          sudo ipvsadm -l --rate
    
    - filesystem: "root"
      path: "/opt/bin/scripts/check_docker_swarm_status.sh"
      mode: 0755
      contents:
        inline: |
          #! /usr/bin/bash
          echo "======================================="
          echo "sudo docker volume ls"
          echo "======================================="
          sudo docker volume ls
          echo "======================================="
          echo "sudo docker node ls"
          echo "======================================="
          sudo docker node ls
          echo "================================================"
          echo "Display docker swarm cluster all node ip address"
          echo "================================================"
          for NODE in $(docker node ls --format '{{.Hostname}}'); do echo -e "${NODE} - $(docker node inspect --format '{{.Status.Addr}}' "${NODE}")"; done
          echo "======================================="
          echo "docker swarm label"
          echo "======================================="
          sudo docker node ls -q | xargs docker node inspect -f '{{ .ID }} [{{ .Description.Hostname }}]: {{ .Spec.Labels }}'
          echo "======================================="
          echo "sudo docker network ls"
          echo "======================================="
          sudo docker network ls
          echo "======================================="
          echo "sudo docker service ls"
          echo "======================================="
          sudo docker service ls
          echo "======================================="
          echo "sudo docker stack ls"
          echo "======================================="
          sudo docker stack ls
    
    - filesystem: "root"
      path: "/opt/bin/scripts/007.docker-swarm-add-label.sh"
      mode: 0755
      contents:
        inline: |
          #! /usr/bin/bash
          sudo docker node update --label-add role=manager worker-020.example.com
          
    - filesystem: "root"
      path: "/etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf"
      mode: 0755
      contents:
        inline: |
          [Service]
          ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
          ExecStartPre=/usr/bin/ln -sf /etc/kubernetes/ssl /etc/kubernetes/pki
    
    - path: /etc/systemd/system/docker.service.d/environment.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          [Service]
          EnvironmentFile=/etc/environment
          
    - path: /etc/systemd/timesyncd.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          [Time]
          NTP=0.flatcar.pool.ntp.org 1.flatcar.pool.ntp.org 2.flatcar.pool.ntp.org 3.flatcar.pool.ntp.org
    
    - path: /etc/ntp.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          server 0.flatcar.pool.ntp.org
          server 1.flatcar.pool.ntp.org
          server 2.flatcar.pool.ntp.org
          server 3.flatcar.pool.ntp.org

          # - Allow only time queries, at a limited rate.
          # - Allow all local queries (IPv4, IPv6)
          restrict default nomodify nopeer noquery limited kod
          restrict 127.0.0.1
          restrict [::1]
          
    - path: /etc/timezone
      filesystem: root
      mode: 0644
      contents:
        inline: Asia/Jakarta
          
    - path: /etc/profile.d/aliases.sh
      filesystem: root
      mode: 0755
      contents:
        inline: |
          check-etcd () { /opt/bin/scripts/check_etcdv3_status.sh ; }
          check-service () { /opt/bin/scripts/check_service_status.sh ; }
          check-iptables () { /opt/bin/scripts/check_iptables_status.sh ; }
          check-ipvsadm () { /opt/bin/scripts/check_ipvsadm_status.sh ; }
          check-swarm () { /opt/bin/scripts/check_docker_swarm_status.sh ; }
          sjf () { sudo journalctl -f ; }
          dim () { docker images ; }
          dps () { docker ps -a ; }
          dl () { docker logs -f $1 & }
          drm () { docker stop $1 && docker rm $1; }
          PATH=$PATH:/opt/bin
          
    - filesystem: "root"
      path: "/etc/kubernetes/cni/net.d/99-loopback.conf" 
      mode: 0644
      contents:
        inline: |
          {
             "cniVersion": "0.3.1",
             "type": "loopback"
          }
          
    - filesystem: "root"
      path: "/etc/kubernetes/cni/net.d/10-containernet.conf"
      mode: 0644
      contents:
        inline: |
          {
            "name": "podnet",
            "type": "flannel",
            "subnetFile": "/var/run/flannel/subnet.env",
            "delegate": {
               "bridge": "cni0",
               "mtu": 1440,
               "forceAddress": true,
               "hairpinMode": true,
               "isDefaultGateway": true
            }
          }
          
    - filesystem: "root"
      path: "/etc/rkt/net.d/99-loopback.conf" 
      mode: 0644
      contents:
        inline: |
          {
             "cniVersion": "0.3.1",
             "type": "loopback"
          }
          
    - filesystem: "root"
      path: "/etc/rkt/net.d/10-containernet.conf"
      mode: 0644
      contents:
        inline: |
          {
            "name": "podnet",
            "type": "flannel",
            "subnetFile": "/var/run/flannel/subnet.env",
            "delegate": {
               "bridge": "cni0",
               "mtu": 1440,
               "forceAddress": true,
               "hairpinMode": true,
               "isDefaultGateway": true
            }
          }
    
    - filesystem: "root"
      path: "/etc/cni/net.d/99-loopback.conf" 
      mode: 0644
      contents:
        inline: |
          {
             "cniVersion": "0.3.1",
             "type": "loopback"
          }
          
    - filesystem: "root"
      path: "/etc/cni/net.d/10-containernet.conf"
      mode: 0644
      contents:
        inline: |
          {
            "name": "podnet",
            "type": "flannel",
            "subnetFile": "/var/run/flannel/subnet.env",
            "delegate": {
               "bridge": "cni0",
               "mtu": 1440,
               "forceAddress": true,
               "hairpinMode": true,
               "isDefaultGateway": true
            }
          }
          
    - filesystem: "root"
      path: "/etc/environment"
      mode: 0644
      contents:
        inline: |
          COREOS_PUBLIC_IPV4=10.9.56.123
          COREOS_PRIVATE_IPV4=10.9.56.123
          COREOS_HOSTNAME=worker-020.example.com
          ETCD_SSL_DIR=/var/lib/etcd/ssl
          ETCD_ENDPOINTS="https://10.9.56.101:2379,https://10.9.56.102:2379,https://10.9.56.103:2379"
          FLANNELD_ETCD_ENDPOINTS="https://10.9.56.101:2379,https://10.9.56.102:2379,https://10.9.56.103:2379"
          FLANNELD_IFACE="ens32"
          FLANNELD_ETCD_PREFIX="/coreos.com/network2"
          LOCKSMITHCTL_ETCD_CAFILE=/etc/ssl/certs/ca.pem
          LOCKSMITHCTL_ETCD_CERTFILE=/var/lib/etcd/ssl/etcd-node.pem
          LOCKSMITHCTL_ETCD_KEYFILE=/var/lib/etcd/ssl/etcd-node-key.pem
          LOCKSMITHCTL_ENDPOINT="https://10.9.56.101:2379,https://10.9.56.102:2379,https://10.9.56.103:2379"
          no_proxy=localhost,127.0.0.0/8,127.0.0.1,::1,10.9.56.101,10.9.56.102,10.9.56.103,manager-01,manager-02,manager-03,manager-01.example.com,manager-02.example.com,manager-03.example.com,example.com,/var/run/docker.sock
    
    - filesystem: "root"
      path: "/etc/systemd/system.conf.d/10-default-env.conf"
      mode: 0644
      contents:
        inline: |
          [Manager]
          DefaultEnvironment="localhost,127.0.0.0/8,127.0.0.1,::1,10.9.56.101,10.9.56.102,10.9.56.103,manager-01,manager-02,manager-03,manager-01.example.com,manager-02.example.com,manager-03.example.com,example.com,/var/run/docker.sock"
    
    # kubelet-config file.
    - filesystem: "root"
      path: "/var/lib/kubelet/kubelet-config.yaml"
      mode: 0644
      contents:
        inline: |
          address: 0.0.0.0
          apiVersion: kubelet.config.k8s.io/v1beta1
          authentication:
            anonymous:
              enabled: false
            webhook:
              enabled: false
            x509:
              clientCAFile: "/etc/kubernetes/ssl/ca.pem"
          authorization:
            mode: AlwaysAllow
          cgroupDriver: cgroupfs
          cgroupsPerQOS: true
          clusterDomain: "cluster.local"
          clusterDNS: 
            - "10.100.0.10"
          containerLogMaxFiles: 5
          containerLogMaxSize: 10Mi
          contentType: application/vnd.kubernetes.protobuf
          cpuCFSQuota: true
          cpuManagerPolicy: none
          cpuManagerReconcilePeriod: 10s
          enableControllerAttachDetach: true
          enableDebuggingHandlers: true
          enforceNodeAllocatable:
            - pods
          eventBurst: 10
          eventRecordQPS: 5
          evictionHard:
            imagefs.available: 15%
            memory.available: 100Mi
            nodefs.available: 10%
            nodefs.inodesFree: 5%
          evictionPressureTransitionPeriod: 5m0s
          failSwapOn: true
          fileCheckFrequency: 20s
          hairpinMode: promiscuous-bridge
          healthzBindAddress: 127.0.0.1
          healthzPort: 10248
          httpCheckFrequency: 20s
          imageGCHighThresholdPercent: 85
          imageGCLowThresholdPercent: 80
          imageMinimumGCAge: 2m0s
          iptablesDropBit: 15
          iptablesMasqueradeBit: 14
          kind: KubeletConfiguration
          tlsCertFile: "/etc/kubernetes/ssl/etcd-node.pem"
          tlsPrivateKeyFile: "/etc/kubernetes/ssl/etcd-node-key.pem"
          RotateCertificates: true
          ServerTLSBootstrap: true
          kubeAPIBurst: 10
          kubeAPIQPS: 5
          makeIPTablesUtilChains: true
          maxOpenFiles: 1000000
          maxPods: 110
          nodeStatusUpdateFrequency: 10s
          oomScoreAdj: -999
          podPidsLimit: -1
          port: 10250
          registryBurst: 10
          registryPullQPS: 5
          runtimeRequestTimeout: 2m0s
          serializeImagePulls: true
          staticPodPath: /etc/kubernetes/manifests
          streamingConnectionIdleTimeout: 4h0m0s
          syncFrequency: 1m0s
          volumeStatsAggPeriod: 1m0s
          podCIDR: "10.9.56.0/24"
          resolvConf: "/run/systemd/resolve/resolv.conf"
    
    # kube-proxy file.
    - filesystem: "root"
      path: "/var/lib/kubeproxy/kube-proxy-config.yaml"
      mode: 0644
      contents:
        inline: |
          apiVersion: kubeproxy.config.k8s.io/v1alpha1
          kind: KubeProxyConfiguration
          bindAddress: 0.0.0.0
          clientConnection:
            kubeconfig: "/etc/kubernetes/ssl/kubeconfig-kube-proxy-worker.yaml"
          mode: "ipvs"
          clusterCIDR: "10.244.0.0/17"
    
    - path: /etc/kubernetes/ssl/kubeconfig-kube-proxy-worker.yaml
      filesystem: root
      mode: 0644
      contents:
        inline: |
          apiVersion: v1
          kind: Config
          preferences: {}
          clusters:
          - name: kubernetes-the-hard-way
            cluster:
              server: https://kubernetes.example.com:6443
              certificate-authority: /etc/kubernetes/ssl/ca.pem
          users:
          - name: kube-proxy
            user:
              client-certificate: /etc/kubernetes/ssl/kube-proxy-worker.pem
              client-key: /etc/kubernetes/ssl/kube-proxy-worker-key.pem
          contexts:
          - context:
              cluster: kubernetes-the-hard-way
              user: kube-proxy
            name: kubernetes-the-hard-way
          current-context: kubernetes-the-hard-way
          
    - filesystem: "root"
      path: "/root/.bashrc"
      mode: 0655
      contents:
        inline: |   
          if [[ $- != *i* ]] ;then
              return 
          fi
          
          alias csysdig="docker run -i -t --rm --privileged -v /var/run/docker.sock:/host/var/run/docker.sock -v /dev:/host/dev -v /proc:/host/proc:ro sysdig/sysdig csysdig -pc"
          alias sysdig="docker run -i -t --rm --privileged -v /var/run/docker.sock:/host/var/run/docker.sock -v /dev:/host/dev -v /proc:/host/proc:ro sysdig/sysdig sysdig"
    
    - filesystem: "root"
      path: "/etc/ssl/certs/ca.pem"
      mode: 0644
      contents:
        inline: |
          -----BEGIN CERTIFICATE-----
          -----END CERTIFICATE-----
    - filesystem: "root"
      path: "/var/lib/etcd/ssl/etcd-node.pem"
      mode: 0644
      contents:
        inline: |
          -----BEGIN CERTIFICATE-----
          -----END CERTIFICATE-----
    - filesystem: "root"
      path: "/var/lib/etcd/ssl/etcd-node-key.pem"
      mode: 0644
      contents:
        inline: |
          -----BEGIN RSA PRIVATE KEY-----
          -----END RSA PRIVATE KEY-----
    - filesystem: "root"
      path: "/etc/coredns/tls/etcd/ca.crt"
      mode: 0644
      contents:
        inline: |
          -----BEGIN CERTIFICATE-----
          -----END CERTIFICATE-----
    - filesystem: "root"
      path: "/etc/coredns/tls/etcd/cert.pem"
      mode: 0644
      contents:
        inline: |
          -----BEGIN CERTIFICATE-----
          -----END CERTIFICATE-----
    - filesystem: "root"
      path: "/etc/coredns/tls/etcd/key.pem"
      mode: 0644
      contents:
        inline: |
          -----BEGIN RSA PRIVATE KEY-----
          -----END RSA PRIVATE KEY-----
